from datetime import datetime
from django.conf import settings
from django_ai_assistant import AIAssistant
from django_ai_assistant.langchain.tools import method_tool
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage

from agents.models import LLMProviderConfig, ChatHistory, AssistantContextFile


def create_dynamic_assistant_class(llm_config: LLMProviderConfig, assistant_id: str = None):
    """
    Cria uma classe AIAssistant din√¢mica baseada no LLMProviderConfig
    """

    class DynamicAIAssistant(AIAssistant):
        id = assistant_id or f"assistant_{llm_config.id}"
        name = llm_config.display_name or f"{llm_config.get_name_display()} - {llm_config.model}"
        instructions = llm_config.instructions or "Voc√™ √© um assistente inteligente."
        model = llm_config.model

        def __init__(self):
            self.llm_config = llm_config
            super().__init__()

        def get_llm(self):
            """Retorna o modelo LLM configurado baseado no LLMProviderConfig"""
            provider = self.llm_config.name

            if provider == "openai":
                return ChatOpenAI(
                    model=self.llm_config.model,
                    temperature=self.llm_config.temperature,
                    max_tokens=self.llm_config.max_tokens,
                    top_p=self.llm_config.top_p,
                    presence_penalty=self.llm_config.presence_penalty,
                    frequency_penalty=self.llm_config.frequency_penalty,
                    openai_api_key=getattr(settings, 'OPENAI_API_KEY', '')
                )
            elif provider == "anthropic":
                return ChatAnthropic(
                    model=self.llm_config.model,
                    temperature=self.llm_config.temperature,
                    max_tokens=self.llm_config.max_tokens,
                    top_p=self.llm_config.top_p,
                    anthropic_api_key=getattr(settings, 'ANTHROPIC_API_KEY', '')
                )
            elif provider == "google":
                return ChatGoogleGenerativeAI(
                    model=self.llm_config.model,
                    temperature=self.llm_config.temperature,
                    max_output_tokens=self.llm_config.max_tokens,
                    top_p=self.llm_config.top_p,
                    google_api_key=getattr(settings, 'GOOGLE_API_KEY', '')
                )
            else:
                # Fallback para OpenAI se provider n√£o reconhecido
                return ChatOpenAI(
                    model=self.llm_config.model,
                    temperature=self.llm_config.temperature,
                    max_tokens=self.llm_config.max_tokens,
                    openai_api_key=getattr(settings, 'OPENAI_API_KEY', '')
                )

        def get_instructions(self):
            """Retorna instru√ß√µes din√¢micas com contexto atual"""
            current_time = datetime.now().strftime('%d/%m/%Y %H:%M:%S')
            base_instructions = self.instructions

            # Adicionar contexto de arquivos
            context_files_content = self._get_context_files_content()
            if context_files_content:
                base_instructions += f"\n\n=== CONTEXTO ADICIONAL ===\n{context_files_content}"

            return f"{base_instructions}\n\nData e hora atual: {current_time}"

        def _get_context_files_content(self):
            """
            Busca e retorna o conte√∫do dos arquivos de contexto ativos
            """
            try:
                context_files = AssistantContextFile.objects.filter(
                    llm_config=self.llm_config,
                    is_active=True,
                    status='ready'
                ).exclude(file_type='pdf').order_by('name')

                if not context_files.exists():
                    return ""

                context_content = []
                total_chars = 0
                max_context_length = 20000

                for file in context_files:
                    if file.extracted_content:
                        file_content = file.extracted_content.strip()

                        if total_chars + len(file_content) > max_context_length:
                            remaining_chars = max_context_length - total_chars
                            if remaining_chars > 100:
                                file_content = file_content[:remaining_chars] + "..."
                                context_content.append(f"**{file.name}:**\n{file_content}")
                            break

                        context_content.append(f"**{file.name}:**\n{file_content}")
                        total_chars += len(file_content) + len(file.name) + 10

                if context_content:
                    result = "\n\n".join(context_content)
                    result += f"\n\n(Contexto baseado em {len(context_content)} arquivo(s) de refer√™ncia)"
                    return result

                return ""

            except Exception as e:
                print(f"Erro ao buscar arquivos de contexto: {e}")
                return ""

    return DynamicAIAssistant


class DjangoAIAssistantService:
    """
    Service que integra LLMProviderConfig com django-ai-assistant
    """

    def __init__(self, llm_config: LLMProviderConfig, assistant_id: str = None):
        self.llm_config = llm_config
        # Criar uma classe din√¢mica AIAssistant
        assistant_class = create_dynamic_assistant_class(llm_config, assistant_id)
        self.assistant = assistant_class()

    def get_llm(self):
        """Delegate para o assistant interno"""
        return self.assistant.get_llm()

    def get_instructions(self):
        """Delegate para o assistant interno"""
        return self.assistant.get_instructions()

class AgentLLMService:
    """
    Service principal que gerencia diferentes provedores LLM usando django-ai-assistant
    """

    def __init__(self, llm_config: LLMProviderConfig, user=None):
        self.llm_config = llm_config

        if llm_config.config_type == 'finance' and user:
            from finance.ai_assistants import FinanceAIAssistant
            self.assistant = FinanceAIAssistant(user=user, llm_config=llm_config)
        elif llm_config.config_type == 'calendar' and user:
            from google_calendar.ai_assistants import GoogleCalendarAIAssistant
            self.assistant = GoogleCalendarAIAssistant(user=user, llm_config=llm_config)
        else:
            self.assistant = DjangoAIAssistantService(llm_config)

    def send_text_message(self, message_content: str, chat_session):
        """
        Envia mensagem de texto usando django-ai-assistant com todas as funcionalidades do OpenAIService

        Args:
            message_content (str): Conte√∫do da mensagem
            chat_session: Sess√£o do chat

        Returns:
            dict: Resposta do assistant ou None em caso de erro
        """
        try:
            print(f"Enviando mensagem via AgentLLMService: {message_content}")

            # VERIFICAR STATUS DA SESS√ÉO - N√£o responder se n√£o permitir AI
            if hasattr(chat_session, 'allows_ai_response') and not chat_session.allows_ai_response():
                print(f"üö´ FILTRADO: Sess√£o {chat_session.from_number} n√£o permite resposta do AI (status: {chat_session.status}) - Assistant n√£o ir√° responder")
                return None

            # Recuperar √∫ltimas 10 mensagens da sess√£o em ordem cronol√≥gica
            last_messages = ChatHistory.objects.filter(session_id=chat_session.from_number, closed=False).order_by("created_at")[:10]

            # Montar contexto do sistema com instru√ß√µes estruturadas
            system_content = self._build_enhanced_system_prompt()

            # PRIMEIRO: Verificar se √© uma solicita√ß√£o relacionada a calend√°rio
            if self.llm_config.config_type == 'calendar':
                system_content += """CONTEXTO IMPORTANTE:
                - Este usu√°rio est√° enviando mensagens via WhatsApp
                - O n√∫mero do WhatsApp √©: {whatsapp_number}
                - Use sempre este n√∫mero nas fun√ß√µes que requerem numero_whatsapp
                - Seja direto e objetivo nas respostas
                - Formate as respostas de forma amig√°vel para WhatsApp
                - Se o usu√°rio solicitar cria√ß√£o de eventos, use os dados fornecidos ou pe√ßa os dados que faltam
                - Para listar eventos, seja conciso mas informativo
                - Para verificar disponibilidade, seja claro sobre conflitos{history_context}"""

            # SEGUNDO: Verificar se √© uma solicita√ß√£o relacionada a finan√ßas
            if self.llm_config.config_type == 'finance':
                pass
                # finance_response = self._try_finance_assistant(message_content, chat_session, last_messages)
                # if finance_response:
                #     return finance_response

            # system_content = self._build_enhanced_system_prompt()

            #
            # # Adicionar informa√ß√£o sobre PDFs que ser√£o anexados
            # pdf_files = self._get_pdf_files()
            # if pdf_files:
            #     pdf_names = [pdf.name for pdf in pdf_files]
            #     system_content += f"\n\n=== DOCUMENTOS PDF ANEXADOS ===\n"
            #     system_content += f"Os seguintes documentos PDF est√£o anexados nesta conversa: {', '.join(pdf_names)}\n"
            #     system_content += "Voc√™ pode referenciar e analisar o conte√∫do destes PDFs diretamente."
            #
            # # Adicionar informa√ß√£o sobre imagens que ser√£o anexadas
            # image_files = self._get_image_files()
            # if image_files:
            #     image_names = [img.name for img in image_files]
            #     system_content += f"\n\n=== IMAGENS ANEXADAS ===\n"
            #     system_content += f"As seguintes imagens est√£o anexadas nesta conversa: {', '.join(image_names)}\n"
            #     system_content += "Voc√™ pode ver e analisar o conte√∫do visual destas imagens diretamente."
            #
            # # Adicionar lista de arquivos dispon√≠veis com URLs
            # available_files = self._get_available_files_with_urls()
            # if available_files:
            #     system_content += f"\n\n=== ARQUIVOS DISPON√çVEIS PARA ENVIO ===\n"
            #     system_content += "Os seguintes arquivos est√£o dispon√≠veis e podem ser enviados usando o formato JSON:\n"
            #     for file_info in available_files:
            #         system_content += f"‚Ä¢ **{file_info['name']}**: {file_info['url']}\n"
            #     system_content += "\nPara enviar qualquer um destes arquivos, use o formato JSON com a URL exata listada acima."
            #
            # # Preparar mensagens usando LangChain format
            messages = []

            # Adicionar instru√ß√µes do sistema
            from langchain.schema import SystemMessage, HumanMessage, AIMessage
            messages.append(SystemMessage(content=system_content))

            # Adicionar hist√≥rico de mensagens
            for msg in last_messages:
                # Garante que content e response nunca sejam None
                human_content = msg.message.get("content") or ""
                ai_response = msg.message.get("response") or ""

                if human_content.strip():
                    messages.append(HumanMessage(content=human_content))

                if ai_response.strip():
                    messages.append(AIMessage(content=ai_response))

            # # Preparar conte√∫do da mensagem atual (com suporte a imagens)
            # user_message_content = []
            #
            # # Adicionar texto da mensagem
            # if message_content and message_content.strip():
            #     user_message_content.append({
            #         "type": "text",
            #         "text": message_content
            #     })
            #
            # # Para suporte a imagens no LangChain, precisamos usar ChatOpenAI com vision
            # current_message_content = message_content
            #
            # # Se h√° imagens, precisamos usar um formato especial para o LangChain
            # image_files = self._get_image_files()
            # if image_files:
            #     # Para LangChain, vamos incluir refer√™ncia √†s imagens no texto
            #     image_refs = []
            #     for img in image_files:
            #         image_refs.append(f"[IMAGEM ANEXADA: {img.name}]")
            #
            #     if image_refs:
            #         current_message_content += f"\n\nImagens anexadas: {', '.join(image_refs)}"
            #
            # # Adicionar mensagem atual
            # if current_message_content and current_message_content.strip():
            #     messages.append(HumanMessage(content=current_message_content))

            # Criar hist√≥rico da mensagem humana
            history = ChatHistory.create(
                session_id=chat_session.from_number,
                content=message_content,
                external_id=chat_session.id,
                response=None
            )

            # Adicionar mensagem atual ao hist√≥rico de mensagens
            messages.append(HumanMessage(content=message_content))

            # Usar django-ai-assistant com suporte a tools
            # O .invoke() do grafo executa com tool calling e hist√≥rico manual
            # thread_id=None evita salvar thread no banco
            graph = self.assistant.as_graph(thread_id=None)

            # Configurar limite de recurs√£o e desabilitar salvamento
            config = {
                "recursion_limit": 50,
                "configurable": {
                    "thread_id": None,  # N√£o salvar thread
                }
            }

            result = graph.invoke({"messages": messages, "input": None}, config=config)
            ai_response = result.get("output", "")

            # Debug: verificar se h√° tool calls na resposta
            if result.get("messages"):
                last_msg = result["messages"][-1]


            # Salvar resposta no hist√≥rico
            history.message['response'] = ai_response
            history.save()

            return ai_response

        except Exception as e:
            print(f"Erro ao comunicar via django-ai-assistant: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    # Manter m√©todo send_message para compatibilidade
    def send_message(self, message_content: str, chat_session, context_data: dict = None):
        """M√©todo de compatibilidade que chama send_text_message"""
        return self.send_text_message(message_content, chat_session)

    def _build_message_history(self, chat_session, current_message):
        """
        Constr√≥i hist√≥rico de mensagens no formato LangChain
        """
        messages = []

        # Adicionar instru√ß√µes do sistema
        system_instructions = self.assistant.get_instructions()
        if system_instructions:
            messages.append(SystemMessage(content=system_instructions))

        # Recuperar √∫ltimas mensagens da sess√£o
        last_messages = ChatHistory.objects.filter(
            session_id=chat_session.from_number,
            closed=False
        ).order_by("created_at")[:10]

        # Adicionar mensagens do hist√≥rico
        for msg in last_messages:
            human_content = msg.message.get("content") or ""
            ai_response = msg.message.get("response") or ""

            if human_content.strip():
                messages.append(HumanMessage(content=human_content))

            if ai_response.strip():
                messages.append(AIMessage(content=ai_response))

        # Adicionar mensagem atual
        if current_message and current_message.strip():
            messages.append(HumanMessage(content=current_message))

        return messages

    def _get_context_files_content(self):
        """
        Busca e retorna o conte√∫do dos arquivos de contexto ativos
        """
        try:
            context_files = AssistantContextFile.objects.filter(
                llm_config=self.llm_config,
                is_active=True,
                status='ready'
            ).exclude(file_type='pdf').order_by('name')

            if not context_files.exists():
                return ""

            context_content = []
            total_chars = 0
            max_context_length = 20000

            for file in context_files:
                if file.extracted_content:
                    file_content = file.extracted_content.strip()

                    if total_chars + len(file_content) > max_context_length:
                        remaining_chars = max_context_length - total_chars
                        if remaining_chars > 100:
                            file_content = file_content[:remaining_chars] + "..."
                            context_content.append(f"**{file.name}:**\n{file_content}")
                        break

                    context_content.append(f"**{file.name}:**\n{file_content}")
                    total_chars += len(file_content) + len(file.name) + 10

            if context_content:
                result = "\n\n".join(context_content)
                result += f"\n\n(Contexto baseado em {len(context_content)} arquivo(s) de refer√™ncia)"
                return result

            return ""

        except Exception as e:
            print(f"Erro ao buscar arquivos de contexto: {e}")
            return ""

    @method_tool
    def list_context_files(self) -> str:
        """
        Lista arquivos de contexto dispon√≠veis para o assistant

        Returns:
            String com lista de arquivos de contexto
        """
        try:
            files = AssistantContextFile.objects.filter(
                llm_config=self.llm_config,
                is_active=True,
                status='ready'
            ).order_by('name')

            if not files.exists():
                return "Nenhum arquivo de contexto dispon√≠vel."

            file_list = ["üìÅ **Arquivos de Contexto Dispon√≠veis:**\n"]

            for i, file in enumerate(files, 1):
                file_info = f"{i}. **{file.name}** ({file.get_file_type_display()})"
                if file.file_size:
                    file_info += f" - {file.get_file_size_display()}"
                file_list.append(file_info)

            return "\n".join(file_list)

        except Exception as e:
            return f"Erro ao listar arquivos: {str(e)}"

    @method_tool
    def get_llm_config_info(self) -> str:
        """
        Retorna informa√ß√µes sobre a configura√ß√£o LLM atual

        Returns:
            String com informa√ß√µes da configura√ß√£o
        """
        try:
            config_info = f"""ü§ñ **Configura√ß√£o LLM Atual:**

                            üìã **Nome:** {self.llm_config.display_name}
                            üè≠ **Provedor:** {self.llm_config.get_name_display()}
                            üß† **Modelo:** {self.llm_config.model}
                            üå°Ô∏è **Temperatura:** {self.llm_config.temperature}
                            üìè **Max Tokens:** {self.llm_config.max_tokens}
                            üéØ **Top-p:** {self.llm_config.top_p}
                            ‚öñÔ∏è **Penalidade Presen√ßa:** {self.llm_config.presence_penalty}
                            üîÑ **Penalidade Frequ√™ncia:** {self.llm_config.frequency_penalty}
                            üìù **Criado em:** {self.llm_config.created_at.strftime('%d/%m/%Y %H:%M')}"""

            return config_info

        except Exception as e:
            return f"Erro ao obter informa√ß√µes da configura√ß√£o: {str(e)}"


    def _get_pdf_files(self):
        """
        Busca arquivos PDF de contexto para envio direto ao modelo
        """
        try:
            pdf_files = AssistantContextFile.objects.filter(
                llm_config=self.llm_config,
                is_active=True,
                status='ready',
                file_type='pdf'
            ).order_by('name')

            return list(pdf_files)

        except Exception as e:
            print(f"Erro ao buscar arquivos PDF: {e}")
            return []

    def _get_image_files(self):
        """
        Busca arquivos de imagem de contexto para envio direto ao modelo
        """
        try:
            image_files = AssistantContextFile.objects.filter(
                llm_config=self.llm_config,
                is_active=True,
                status='ready',
                file_type__in=['jpg', 'png', 'gif', 'webp']
            ).order_by('name')

            return list(image_files)

        except Exception as e:
            print(f"Erro ao buscar arquivos de imagem: {e}")
            return []

    def _get_available_files_with_urls(self):
        """
        Busca todos os arquivos de contexto ativos e suas URLs para envio
        """
        try:
            # Buscar todos os arquivos de contexto ativos e prontos
            all_files = AssistantContextFile.objects.filter(
                llm_config=self.llm_config,
                is_active=True,
                status='ready'
            ).order_by('name')

            available_files = []

            for file in all_files:
                file_info = {
                    'name': file.name,
                    'type': file.file_type,
                }

                # Tentar obter URL do arquivo
                try:
                    if hasattr(file.file, 'url'):
                        # Se o arquivo tem URL direta (ex: storage p√∫blico)
                        file_url = file.file.url

                        # Converter URL relativa em absoluta se necess√°rio
                        if file_url.startswith('/'):
                            # Construir URL absoluta usando settings
                            from django.conf import settings
                            base_url = getattr(settings, 'SITE_URL', 'http://localhost:8000')
                            file_url = f"{base_url.rstrip('/')}{file_url}"

                        file_info['url'] = file_url
                        available_files.append(file_info)

                    elif hasattr(file.file, 'path'):
                        # Se s√≥ tem caminho local, criar URL baseada no Django MEDIA_URL
                        from django.conf import settings

                        # Obter o caminho relativo do arquivo
                        full_path = file.file.path
                        media_root = getattr(settings, 'MEDIA_ROOT', '')

                        if media_root and full_path.startswith(media_root):
                            relative_path = full_path[len(media_root):].lstrip('/')
                            media_url = getattr(settings, 'MEDIA_URL', '/media/')

                            # Construir URL completa
                            base_url = getattr(settings, 'SITE_URL', 'http://localhost:8000')
                            file_url = f"{base_url.rstrip('/')}{media_url.rstrip('/')}/{relative_path}"

                            file_info['url'] = file_url
                            available_files.append(file_info)

                except Exception as url_error:
                    print(f"Erro ao obter URL do arquivo {file.name}: {url_error}")
                    continue

            return available_files

        except Exception as e:
            print(f"Erro ao buscar arquivos dispon√≠veis: {e}")
            return []

    def _process_structured_response(self, ai_response):
        """
        Processa resposta da IA em formato JSON estruturado
        Formato esperado: {"text": "mensagem", "file": "url_do_arquivo"}
        """
        import json
        import re

        try:
            # Tentar encontrar JSON na resposta
            json_pattern = r'\{[^{}]*"text"[^{}]*"file"[^{}]*\}'
            json_match = re.search(json_pattern, ai_response)

            if json_match:
                json_str = json_match.group(0)
                try:
                    parsed_data = json.loads(json_str)

                    # Validar se tem os campos necess√°rios
                    if "text" in parsed_data and "file" in parsed_data:
                        text = parsed_data.get("text", "").strip()
                        file_url = parsed_data.get("file", "").strip()

                        # Validar se ao menos um dos campos n√£o est√° vazio
                        if text or file_url:
                            result = {
                                "type": "structured",
                                "text": text,
                                "file": file_url
                            }
                            print(f"Resposta estruturada detectada: texto='{text[:50]}...', arquivo='{file_url}'")
                            return result

                except json.JSONDecodeError:
                    pass

            # Tentar formato mais flex√≠vel sem JSON estrito
            text_match = re.search(r'"?text"?\s*:\s*"([^"]*)"', ai_response)
            file_match = re.search(r'"?file"?\s*:\s*"([^"]*)"', ai_response)

            if text_match or file_match:
                text = text_match.group(1) if text_match else ""
                file_url = file_match.group(1) if file_match else ""

                if text or file_url:
                    result = {
                        "type": "structured",
                        "text": text.strip(),
                        "file": file_url.strip()
                    }
                    print(f"Resposta estruturada (flex√≠vel) detectada: texto='{text[:50]}...', arquivo='{file_url}'")
                    return result

            return None

        except Exception as e:
            print(f"Erro ao processar resposta estruturada: {e}")
            return None

    def _build_enhanced_system_prompt(self):
        """
        Constr√≥i um prompt de sistema melhorado que interpreta melhor as instru√ß√µes do usu√°rio
        """
        current_time = datetime.now().strftime('%d/%m/%Y %H:%M:%S')

        # Come√ßar com se√ß√£o de identidade e contexto temporal
        system_prompt = f"=== CONTEXTO E IDENTIDADE ===\n"
        system_prompt += f"Data e hora atual: {current_time}\n\n"

        # Processar e estruturar as instru√ß√µes do usu√°rio
        enhanced_instructions = self.llm_config.instructions or "Voc√™ √© um assistente inteligente."

        return enhanced_instructions

    def _enhance_user_instructions(self, raw_instructions):
        """
        Analisa e melhora as instru√ß√µes fornecidas pelo usu√°rio para torn√°-las mais efetivas
        """
        instructions = raw_instructions.strip()

        # Detectar padr√µes comuns e adicionar estrutura
        enhanced = ""

        # Se as instru√ß√µes cont√™m caracter√≠sticas de personalidade
        if any(keyword in instructions.lower() for keyword in ['voc√™ √©', 'atue como', 'comporte-se', 'seja', 'comportamento']):
            enhanced += "IDENTIDADE E PERSONALIDADE:\n"
            enhanced += f"{instructions}\n\n"

            enhanced += "APLICA√á√ÉO DAS INSTRU√á√ïES:\n"
            enhanced += "‚Ä¢ Mantenha essa identidade consistentemente durante toda a conversa\n"
            enhanced += "‚Ä¢ Todas as suas respostas devem refletir essa personalidade\n"
            enhanced += "‚Ä¢ Se houver conflito entre instru√ß√µes, priorize o comportamento descrito acima\n"

        # Se as instru√ß√µes cont√™m procedimentos ou regras espec√≠ficas
        elif any(keyword in instructions.lower() for keyword in ['sempre', 'nunca', 'quando', 'se', 'deve', 'n√£o deve', 'regra', 'procedimento']):
            enhanced += "REGRAS E PROCEDIMENTOS:\n"
            enhanced += f"{instructions}\n\n"

            enhanced += "CUMPRIMENTO DAS REGRAS:\n"
            enhanced += "‚Ä¢ Siga rigorosamente todas as regras listadas acima\n"
            enhanced += "‚Ä¢ Verifique cada resposta antes de envi√°-la para garantir conformidade\n"
            enhanced += "‚Ä¢ Em caso de d√∫vida, pe√ßa esclarecimentos ao usu√°rio\n"

        # Se as instru√ß√µes cont√™m conhecimento espec√≠fico ou especializa√ß√£o
        elif any(keyword in instructions.lower() for keyword in ['especialista', 'conhecimento', '√°rea', 'dom√≠nio', 'expert', 'foco']):
            enhanced += "√ÅREA DE ESPECIALIZA√á√ÉO:\n"
            enhanced += f"{instructions}\n\n"

            enhanced += "APLICA√á√ÉO DO CONHECIMENTO ESPECIALIZADO:\n"
            enhanced += "‚Ä¢ Use seu conhecimento especializado para fornecer respostas detalhadas e precisas\n"
            enhanced += "‚Ä¢ Cite fontes ou refer√™ncias quando apropriado\n"
            enhanced += "‚Ä¢ Explique conceitos complexos de forma acess√≠vel quando necess√°rio\n"

        # Caso geral - instru√ß√µes simples ou outras
        else:
            enhanced += "INSTRU√á√ïES PERSONALIZADAS:\n"
            enhanced += f"{instructions}\n\n"

            enhanced += "INTERPRETA√á√ÉO E APLICA√á√ÉO:\n"
            enhanced += "‚Ä¢ Interprete essas instru√ß√µes de forma ampla e consistente\n"
            enhanced += "‚Ä¢ Aplique o esp√≠rito das instru√ß√µes, n√£o apenas a letra\n"
            enhanced += "‚Ä¢ Adapte sua abordagem conforme necess√°rio para cumprir os objetivos\n"

        # Adicionar lembretes importantes
        enhanced += "\nLEMBRETE IMPORTANTE:\n"
        enhanced += "Estas instru√ß√µes t√™m prioridade sobre comportamentos padr√£o. Certifique-se de que cada resposta est√° alinhada com as diretrizes acima."

        return enhanced




# Factory function para criar services baseados no provider
def create_llm_service(llm_config: LLMProviderConfig, user=None):
    """
    Factory function para criar o service apropriado baseado na configura√ß√£o LLM

    Args:
        llm_config: Configura√ß√£o do LLM
        use_django_ai_assistant: Se deve usar django-ai-assistant (padr√£o: True)
        user: Usu√°rio para assistentes especializados (finance, calendar)

    Returns:
        Service instance apropriado
    """
    return AgentLLMService(user=user, llm_config=llm_config)
